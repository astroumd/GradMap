{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Line Data Cubes in Astronomy - Part 1 \n",
    "\n",
    "In this notebook we will introduce spectral line data cubes in astronomy. They are a convenient way to store many spectra at points in the sky. Much like having a spectrum at every pixel in a CCD. In this Part 1 we will keep it as much \"pure python\", and not use astronomical units and just work in \"pixel\" or \"voxel\" space.   In Part2 we will repeat the process with a more astronomy rich set of modules that you will have to install.\n",
    "\n",
    "They normally are presented as a [FITS](https://en.wikipedia.org/wiki/FITS) file, with two sky coordinates (often Right Ascension and Declination) and one spectral coordinate (either an observing frequency or wavelength, and when there is a known spectral line, you can reference using this line with a velocity using the doppler effect). For radio data, such as ALMA and the VLA, we often use GHz or MHz. For optical data we often use the Angstrom (the visible range is around 4000 - 8000 Angstrom, or 400 - 800 nm).\n",
    "\n",
    "![Example Cube](../data/cube_dims_and_cell.png \"just an example cube\")\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To introduce the concepts of spectral line data cubes\n",
    "\n",
    "- Definition of image cube\n",
    "- Data representation image cube\n",
    "- Introduction to galaxy rotating disks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first line of code is actually not real python code, but a magic ipython command, to make sure that the standard plotting commands are going to be displayed within the browser. You will see that happen below. The cube figure about is just a static PNG file.\n",
    "\n",
    "As we will progress learning about the data and how to explore it further, you will notice this decision making process  throughout this notebook.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [astropy](http://www.astropy.org/) package has an I/O package to simplify reading and writing a number of popular formats common in astronomy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdu = fits.open('../data/ngc6503.cube.fits')\n",
    "print(len(hdu))\n",
    "print(hdu[0])\n",
    "print(hdu[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A FITS file consists of a series of Header-Data-Units (HDU). Usually there is only one, representing the image. But this file has two. For now, we're going to ignore the second, which is a special table and in this example happens to be empty anyways.  Each HDU has a header, and data.  The data in this case is a numpy array, and represents the image (cube):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = hdu[0].header\n",
    "d = hdu[0].data\n",
    "print(d.shape, d.min(), d.max(), d.mean(), np.median(d), d.std())\n",
    "print(\"Signal/Noise  (S/N):\",d.max()/d.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shape (1,89,251,371) we can see this image is actually 4 dimensional, although the 4th dimension is dummy.  There are 371 pixels along X, 251 along Y, and 89 slices or spectral channels. It looks like the noise is around 0.00073 and a peak value 0.017, thus a signal to noise of a little over 23, so quite strong.\n",
    "\n",
    "In python you can remove that dummy 4th axis, since we are not going to use it any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# printing out the (dictionary) header:\n",
    "print(h.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = d.squeeze()\n",
    "print(d.shape)\n",
    "# nz=d.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you were wondering about that 4th redundant axis. In astronomy we sometimes observe more than one type of radiation. Since waves are polarized, we can have up to 4 so called Stokes parameters, describing the waves as e.g. linear or circular polarized radiation.  We will ignore that here, but they are sometimes stored in that 4th dimension. Sometimes they are stored as separate cubes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting some basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = 38\n",
    "z = 45         # the mystery blob\n",
    "im = d[z,:,:]  #   im = d[z]     also works\n",
    "#im = d[z, 50:110, 210:270]\n",
    "#im = d[z, 100:150, 140:180]\n",
    "plt.imshow(im,origin=['Lower'])\n",
    "plt.colorbar()\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 89 channels (slices) in this cube, numbered 0 through 88 in the usual python sense. Pick a few other slices by changing the value in \n",
    "**z=** and notice that the first few and last few appear to be just noise and that the V-shaped signal changes shape through the channels. Perhaps you should not be surprised that these are referred to as butterfly diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at a histogram of all the data (histogram needs a 1D array)\n",
    "d1 = d.ravel()                 # ravel() doesn't make a new copy of the array, saving memory\n",
    "print(d1.shape)\n",
    "(n,b,p) = plt.hist(d1, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the histogram is on the left in the plot, and we already saw the maximum data point is 0.0169835.\n",
    "\n",
    "So let us plot the vertical axis logarithmically, so we can better see what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(n,b,p) = plt.hist(d1,bins=100,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pick a slice and make a histogram and print the mean and standard deviation of the signal in that slice\n",
    "z=0\n",
    "imz = d[z,:,:].flatten()\n",
    "(n,b,p) = plt.hist(imz,bins=100)\n",
    "print(imz.mean(), imz.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** : observe by picking some values of **z** that the noise seems to vary a little bit from one end of the band to the other.  Store the noise in channel 0 and 88 in variables sigma0 and sigma88:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed the RMS in a channel, we might as well compute them for all channels!\n",
    "\n",
    "We are also comparing this channel based RMS with the single cube RMS that we determined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nchan = d.shape[0]\n",
    "channel = np.arange(nchan)\n",
    "rms = np.zeros(nchan)\n",
    "peak = np.zeros(nchan)\n",
    "cuberms = np.zeros(nchan) + d.std()\n",
    "for z in range(nchan):\n",
    "    imz = d[z,:,:].flatten()\n",
    "    rms[z] = imz.std()\n",
    "    peak[z] = imz.max()\n",
    "plt.plot(channel,rms,label='chan_rms')\n",
    "#plt.plot(channel,peak,label='peak')\n",
    "plt.plot(channel,cuberms,label='cube_rms',color='black')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Channel\")\n",
    "plt.ylabel(\"RMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  can you think of a better way to compute the RMS as function of channel (the blue line) and *not* have it depend so much on where there is signal?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function for slice statistics\n",
    "import numpy.ma as ma\n",
    "def robust(d, method=0, ns=4.0, rf=1.5):\n",
    "    if method==0:\n",
    "        return d.std()\n",
    "    elif method==1:\n",
    "        m = d.mean()\n",
    "        s = d.std()\n",
    "        d1 = ma.masked_outside(d,m-ns*s,m+ns*s)\n",
    "        return d1.std()\n",
    "    elif method==2:\n",
    "        # assume mean is close enough to zero and no absorbtion\n",
    "        m = d.min()\n",
    "        d1 = ma.masked_outside(d,m,-m)\n",
    "        return d1.std()\n",
    "    elif method==3:\n",
    "        n = len(d)\n",
    "        d.sort()\n",
    "        q1 = d[n//4]\n",
    "        q3 = d[(3*n)//4]\n",
    "        D = q3-q1\n",
    "        d1 = ma.masked_outside(d,q1-rf*D,q3+rf*D)\n",
    "        return d1.std()\n",
    "    else:\n",
    "        return d.std()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nchan = d.shape[0]\n",
    "channel = np.arange(nchan)\n",
    "rms0 = np.zeros(nchan)\n",
    "rms1 = np.zeros(nchan)\n",
    "rms2 = np.zeros(nchan)\n",
    "rms3 = np.zeros(nchan)\n",
    "rms4 = np.zeros(nchan)\n",
    "rms5 = np.zeros(nchan)\n",
    "peak = np.zeros(nchan)\n",
    "cuberms = np.zeros(nchan) + d.std()\n",
    "for z in range(nchan):\n",
    "    imz = d[z,:,:].flatten()\n",
    "    imz4 = d[z,0:80,280:355].flatten()\n",
    "    imz5 = d[z,170:250,0:120].flatten()\n",
    "    rms0[z] = robust(imz,0)\n",
    "    rms1[z] = robust(imz,1,ns=4.0)\n",
    "    rms2[z] = robust(imz,2)\n",
    "    rms3[z] = robust(imz,3,rf=1.5)\n",
    "    rms4[z] = robust(imz4,0)\n",
    "    rms5[z] = robust(imz5,0)\n",
    "    # peak[z] = imz.max()\n",
    "plt.plot(channel,rms0,label='chan_rms0')\n",
    "plt.plot(channel,rms1,label='chan_rms1')\n",
    "plt.plot(channel,rms2,label='chan_rms2')\n",
    "plt.plot(channel,rms3,label='chan_rms3')\n",
    "plt.plot(channel,rms4,label='chan_rms_lr')\n",
    "plt.plot(channel,rms5,label='chan_rms_tl')\n",
    "# plt.plot(channel,peak,label='peak')\n",
    "plt.plot(channel,cuberms,label='cube_rms',color='black')\n",
    "plt.legend(loc='best',fontsize='small')\n",
    "plt.xlabel(\"Channel\")\n",
    "plt.ylabel(\"RMS\")\n",
    "plt.savefig(\"n6503_rms.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next we are interested in the Signal/Noise per channel where is there is no signal. This is clear in the first few and last channels. Recall that in the absence of real signal the peak will always be a few times sigma, purely based on the error function behavior of the distribution of gaussian noise. In our case something like $4\\sigma$. For small maps more like $3\\sigma$, for really big maps or cubes $5\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rms0 = rms[0:15].mean()\n",
    "rms1 = rms[88-13:88].mean()\n",
    "cuberms = np.zeros(nchan) + 0.5*(rms0+rms1)\n",
    "sn0 = peak/rms0\n",
    "sn1 = peak/rms1\n",
    "plt.plot(channel,sn0,label='S/N(low)')\n",
    "plt.plot(channel,sn1,label='S/N(high)')\n",
    "plt.plot(channel[0:15],np.zeros(15)+1,color='black',label='edge')\n",
    "plt.plot(channel[88-13:88],np.zeros(13)+1,color='black')\n",
    "plt.legend(loc='best')\n",
    "print(rms0,rms1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1=peak[0:15]/rms[0:15]\n",
    "s2=peak[75:88]/rms[75:88]\n",
    "print(\"First few channels:\",s1.mean(),s1.std())\n",
    "print(\"Last  few channels:\",s2.mean(),s2.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian noise probability distribution is given by\n",
    "$$\n",
    "P(x) =  { 1 \\over {\\sigma \\sqrt{2\\pi}}} {e^{- { x^2 \\over {2 \\sigma^2}}}}\n",
    "$$\n",
    "where the mean is 0 and RMS is $\\sigma$. This function is normalized, integrated over *x* results in 1.\n",
    "\n",
    "Lets do a simulation to see if we can understand the S/N in this plot. We will need the error function to compute the chance of being in the tail part of the gaussian. The error function is defined as:\n",
    "$$\n",
    "erf(x) =  { {2}\\over{\\sqrt{\\pi}}}      \\int_0^x  e^{-t^2} dt\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def pnoise(n):\n",
    "    \"\"\" chance measuring noise of n sigma\"\"\"\n",
    "    return 0.5*math.erfc(n/math.sqrt(2.0))\n",
    "\n",
    "nsample = 10000\n",
    "g = np.random.normal(size=nsample)\n",
    "sn = g.max()/g.std()\n",
    "print(\"S/N: \",sn)\n",
    "print(\"1/P(S/N)=\",1/pnoise(sn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1/chance for a +1,2,3 sigma detection\n",
    "print(1/pnoise(1.0))\n",
    "print(1/pnoise(2.0))\n",
    "print(1/pnoise(3.0))\n",
    "print(1/pnoise(4.0))\n",
    "print(1/pnoise(5.0))\n",
    "nxy = d.shape[1]*d.shape[2]\n",
    "print(\"Number of pixels in a map:\",nxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "peakpos = (175,125)     # some strong point in the disk of the galaxy\n",
    "peakpos = (231,80)     # the mystery blob?\n",
    "#peakpos = (310,50)      # no signal\n",
    "spectrum = d[:,peakpos[1],peakpos[0]]\n",
    "sns = spectrum.max()/rms[0:15].mean()\n",
    "zero = spectrum * 0.0\n",
    "plt.plot(channel,spectrum,'o-',markersize=2)\n",
    "plt.plot(channel,zero)\n",
    "plt.plot(channel,cuberms,'r--',label='1$\\sigma$')\n",
    "plt.plot(channel,-cuberms,'r--')\n",
    "plt.title(\"Spectrum at position %s  S/N=%.3g\" % (str(peakpos),sns))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the noise correlated? Hanning smoothing is often used to increase the S/N. Test this by taking the differences between neighboring signals and computing the RMS of this \"signal\". If noise is normal and not correllated, the ratio of this RMS to the original RMS of the signal should be $\\sqrt{2}$. Pick a point where there is no obvious signal, such as the (310,50) position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdelt3 = h['CDELT3']\n",
    "crval3 = h['CRVAL3']\n",
    "crpix3 = h['CRPIX3']\n",
    "restfreq=h['RESTFREQ']\n",
    "freq = (channel-crpix3+1)*cdelt3 + crval3      # at the reference pixel we get the reference value\n",
    "c = 299792.458\n",
    "channelv = (1.0-freq/restfreq) * c          # convert to doppler velocity in km/s\n",
    "print(\"min/max/dv:\",channelv[0],channelv[nchan-1],channelv[0]-channelv[1])\n",
    "plt.plot(channelv,spectrum,'o-',markersize=2)\n",
    "plt.plot(channelv,zero)\n",
    "plt.plot(channelv,cuberms,'r--',label='1$\\sigma$')\n",
    "plt.plot(channelv,-cuberms,'r--')\n",
    "plt.title(\"Spectrum at positon %s  S/N=%.3g\" % (str(peakpos),sns))\n",
    "plt.legend()\n",
    "plt.xlabel(\"velocity (km/s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving a descriptive spectrum using pickle\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "   \n",
    "# construct a descriptive spectrum \n",
    "sp = {}\n",
    "sp['z'] = channelv\n",
    "sp['i'] = spectrum\n",
    "sp['zunit'] = 'km/s'\n",
    "sp['iunit'] = h['BUNIT'] \n",
    "sp['xpos']  = peakpos[0]\n",
    "sp['ypos']  = peakpos[1]\n",
    "    \n",
    "# write it\n",
    "pfile = \"n6503-sp.p\" \n",
    "pickle.dump(sp,open(pfile,\"wb\"))\n",
    "print(\"Wrote spectrum\",pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dspectrum = spectrum[1:] - spectrum[:-1]\n",
    "# dspectrum = np.diff(spectrum)                      # this also works (but look up documentation!)\n",
    "rms1 = dspectrum.std()\n",
    "rms0 = spectrum.std()\n",
    "print(rms1,\"/\",rms0,\"=\",rms1/rms0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of the noise you see here should be $\\sqrt{2}$, but let's see for a typical normal distribution how close we are to $\\sqrt{2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "nsample = 100000\n",
    "g = np.random.normal(10.0,5.0,nsample)\n",
    "delta = np.diff(g)\n",
    "gh=plt.hist([g,delta],32)\n",
    "print(g.std(),delta.std(),delta.std()/g.std()/math.sqrt(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing a cube to enhance the signal to noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "import scipy.ndimage.filters as filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = 0\n",
    "print(\"old rms\",rms[z])\n",
    "sigma = 2.0\n",
    "ds1 = filters.gaussian_filter(d[z],sigma)                    # ds1 is a single smoothed slice\n",
    "print(\"new:\",ds1.shape, ds1.mean(), ds1.std())\n",
    "plt.imshow(ds1,origin=['Lower'])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the noise is indeed lower than your earlier value of sigma0.   We only smoothed one single slice, but we actually need to smooth the whole cube.  Each slice  with sigma, but we can optionally also smooth in the spectral dimension a little bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = filters.gaussian_filter(d,[1.0,sigma,sigma])              # ds is a smoothed cube \n",
    "plt.imshow(ds[z],origin=['Lower'])\n",
    "plt.colorbar()\n",
    "print(ds[z].std())\n",
    "print(ds.max(),ds.max()/ds1.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, although the peak value was lowered a bit due to the smoothing, the signal to noise has increased from the original cube. So, the signal should stand out a lot better.\n",
    "\n",
    "**Exercise** : Observe a subtle difference in the last two plots. Can you see what happened here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  sigma0 is the noise in the original cube\n",
    "sigma0 = rms0\n",
    "nsigma = 0.0\n",
    "dm = ma.masked_inside(d,-nsigma*sigma0,nsigma*sigma0)\n",
    "print(dm.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mom0 = dm.sum(axis=0)\n",
    "plt.imshow(mom0,origin=['Lower'])\n",
    "plt.colorbar()\n",
    "#\n",
    "(ypeak,xpeak) = np.unravel_index(mom0.argmax(),mom0.shape)\n",
    "print(\"PEAK at location:\",xpeak,ypeak,mom0.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spectrum2 = ds[:,ypeak,xpeak]\n",
    "plt.plot(channel,spectrum2)\n",
    "plt.plot(channel,zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mom0s = ds.sum(axis=0)\n",
    "plt.imshow(mom0s,origin=['Lower'])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Velocity fields\n",
    "\n",
    "The mean velocity is defined as the first moment\n",
    "\n",
    "$$\n",
    "<V> = {\\Sigma{(v.I)} \\over \\Sigma{(I)} }\n",
    "$$\n",
    "\n",
    "also known as an intensity weighted mean velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nz = d.shape[0]\n",
    "vchan = np.arange(nz).reshape(nz,1,1)\n",
    "vsum = vchan * d\n",
    "vmean = vsum.sum(axis=0)/d.sum(axis=0)\n",
    "print(\"MINMAX\",vmean.min(),vmean.max())\n",
    "plt.imshow(vmean,origin=['Lower'],vmin=0,vmax=88)\n",
    "#plt.imshow(vmean,origin=['Lower'])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can recognize an area of coherent motions (the red and blue shifted sides of the galaxy), there is a lot of noise in this image. Looking at the math, we are dividing two numbers, both of which can be noise, so the outcome can be \"anything\".  If anything, it should be a value between 0 and 88, so we could mask for that and see how that looks.\n",
    "\n",
    "Let us first try to see how the smoothed cube looked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nz = ds.shape[0]\n",
    "vchan = np.arange(nz).reshape(nz,1,1)\n",
    "vsum = vchan * ds\n",
    "vmean = vsum.sum(axis=0)/ds.sum(axis=0)\n",
    "print(vmean.shape,vmean.min(),vmean.max())\n",
    "plt.imshow(vmean,origin=['Lower'],vmin=0,vmax=88)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although more coherent, there are still bogus values outside the image of the galaxy. So we are looking for a hybrid of the two methods.  In the smooth cube we saw the signal to noise is a lot better defined, so we will define areas in the cube where the signal to noise is high enough and use those in the original high resolution cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is all messy , we need a better solution, a hybrid of the two:\n",
    "noise = ds[0:5].flatten()\n",
    "(n,b,p) = plt.hist(noise,bins=100)\n",
    "print(noise.mean(), noise.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma0 = noise.std()\n",
    "nsigma = 5.0\n",
    "cutoff = sigma0*nsigma\n",
    "dm = ma.masked_inside(ds,-cutoff,cutoff)    # assumes mean is close to 0\n",
    "print(cutoff,dm.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dm2=ma.masked_where(ma.getmask(dm),d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vsum = vchan * dm2\n",
    "vmean = vsum.sum(axis=0)/dm2.sum(axis=0)\n",
    "print(vmean.min(),vmean.max())\n",
    "plt.imshow(vmean,origin=['Lower'],vmin=0,vmax=88)\n",
    "plt.colorbar()\n",
    "# some guess of where the major axis of the galaxy is\n",
    "s = [50,55,300,205]\n",
    "plt.plot([s[0],s[2]],[s[1],s[3]], 'k-', lw=2)\n",
    "\n",
    "print(vmean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila, now this looks a lot better, although only velocies between 0 and 88 are possible. Any other values are no doubt due to a noisy division of two numbers.  Experiment with a different value of **nsigma** here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving your output\n",
    "\n",
    "This result is now stored in the **vmean** numpy array. But how do we make this information persistent?\n",
    "\n",
    "The answer is again in FITS format. Where the **fits.open()** function would retrieve a Header and Data \n",
    "(or series of), we need to construct a Header with this Data and write it using **fits.writeto()**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the old hdu[0] is still available, but points to a 3D cube, so lets just try and make it 2D\n",
    "hv = h.copy()\n",
    "hv['NAXIS'] = 2\n",
    "#   cannot write yet: complains about illegal axes\n",
    "hv.remove('NAXIS3')\n",
    "hv.remove('NAXIS4')\n",
    "print(type(vmean))\n",
    "print(vmean.shape)\n",
    "print(h['BITPIX'])\n",
    "#   cannot write yet: complains about masking\n",
    "vmean0 = ma.filled(vmean,0.0)\n",
    "#   finally write it successfully\n",
    "fits.writeto('n6503-vmean.fits',vmean0,hv,clobber=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation Curves\n",
    "\n",
    "The simplest way perhaps is to measure the velocities along the major axis. Here's a crummy way to extract it along the whole major axis, just to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "s = [20,30,300,210]\n",
    "x0, y0 = s[0],s[1]\n",
    "x1, y1 = s[2],s[3]\n",
    "num = 200\n",
    "x, y = np.linspace(x0, x1, num), np.linspace(y0, y1, num)\n",
    "z = scipy.ndimage.map_coordinates(np.transpose(vmean), np.vstack((x,y)))\n",
    "plt.imshow(vmean,origin=['Lower'],vmin=0,vmax=89)\n",
    "plt.plot([x0, x1], [y0, y1], 'ro-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receding and Approaching rotation curve\n",
    "\n",
    "And here the version of a rotation curve along the receding and approaching sides. This is generally a better idea, not only to get an idea if the two sides are compatible.\n",
    "\n",
    "N6503, according to http://ned.ipac.caltech.edu/ is located at 17h49m26.4s +70d08m40s \n",
    "\n",
    "We can either use ds9 to use the cursor at this RA/DEC and read off the pixel value (but we should note each pixel is 4\") \n",
    "\n",
    "I got about :  163.4  123.1\n",
    "\n",
    "This is based on the first pixel in the lower left of the image being (1,1)\n",
    "\n",
    "For python we need a (0,0) based system, so this would be (162.3,122.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c1=h['crpix1']\n",
    "c2=h['crpix2']\n",
    "print(c1,c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "center = [163.5,123.2]   # orig: 5.4\n",
    "center = [164.5,124.2]  #  10\n",
    "center = [162.5,122.2]     #  wow, about same\n",
    "center = [161.5,121.2]\n",
    "center = [162.3,122.1]\n",
    "length = 150.0\n",
    "pa     = 120.0\n",
    "cosp = math.cos(pa*math.pi/180.0) \n",
    "sinp = math.sin(pa*math.pi/180.0) \n",
    "x0,y0 = center\n",
    "#\n",
    "x1 = x0 - length * sinp         # redshifted side (receding)\n",
    "y1 = y0 + length * cosp\n",
    "x2 = x0 + length * sinp         # blue shifted side (approaching)\n",
    "y2 = y0 - length * cosp\n",
    "num = 200\n",
    "x1s, y1s = np.linspace(x0, x1, num), np.linspace(y0, y1, num)\n",
    "x2s, y2s = np.linspace(x0, x2, num), np.linspace(y0, y2, num)\n",
    "z1 = scipy.ndimage.map_coordinates(np.transpose(vmean), np.vstack((x1s,y1s)))\n",
    "z2 = scipy.ndimage.map_coordinates(np.transpose(vmean), np.vstack((x2s,y2s)))\n",
    "plt.imshow(vmean,origin=['Lower'],vmin=0,vmax=89)\n",
    "plt.colorbar()\n",
    "plt.plot([x0, x1], [y0, y1], 'ro-')\n",
    "plt.plot([x0, x2], [y0, y2], 'bo-')\n",
    "plt.plot([c1,c1],[c2,c2],'yo-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(z1)\n",
    "plt.plot(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v1 = z1 - z1[0]\n",
    "v2 = z2[0]-z2\n",
    "print(\"Offsets: v1,v2=\",z1[0],z2[0])\n",
    "plt.plot(v1)\n",
    "plt.plot(v2)\n",
    "iflat = 50\n",
    "print(v1[iflat:].mean(), v1[iflat:].std())\n",
    "print(v2[iflat:].mean(), v2[iflat:].std())\n",
    "print(v1[iflat:].mean()-v2[iflat:].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these two rotation curves with Figure 12 in the Greisen et al. (2009) paper:\n",
    "![fig12](../data/fig12.png \"Fig12 from Greisen et al (2009)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epilogue\n",
    "\n",
    "Some of the pure python constructs that we discussed here, notably masking and smoothing, become cumbersome. Also the absence of astronomical units (instead of pixels) can be cumbersome. In the advanced case we will use some community developed code that makes working with such spectral line image cubes a lot easier. For the particular problem of rotation curves there are also non-python solutions available, see the ASCL based list below. \n",
    "  \n",
    "## ASCL\n",
    "\n",
    "The is an online registry, [ASCL](http://ascl.net), where you can search for astrophysics codes. For example, to analyse velocity fields to extract rotation curves there are several:\n",
    "\n",
    "* [NEMO](http://ascl.net/1010.051) look for the **rotcur** program\n",
    "* [GIPSY](http://ascl.net/1109.018) look for the **rotcur** program\n",
    "* [AIPS](http://ascl.net/9911.003) look for the **CUBIT** and **GAL** programs\n",
    "* [TiRiFiC](http://ascl.net/1208.008) uses GIPSY, 3D fit\n",
    "* [FAT](http://ascl.net/1507.011) TiRiFiC was deemed hard to use, so this is a frontend to make it automated\n",
    "* [3D-Barolo](http://ascl.net/1507.001) 3D fit\n",
    "* [velfit](http://ascl.net/1010.021) fitting a velocity field\n",
    "* [diskfit](http://ascl.net/1209.011) fitting a velocity field as well as mass (light) distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papers\n",
    "\n",
    "The data cube we have used in this notebook has been provided by Eric Greisen (NRAO), and his 2009 paper discussed results in detail: \n",
    "http://adsabs.harvard.edu/abs/2009AJ....137.4718G\n",
    "\n",
    "Data are also available on Greisen's [ftp](ftp://ftp.aoc.nrao.edu/staff/egreisen/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epilogue\n",
    "\n",
    "Some of the pure python constructs that we discussed here, notably masking and smoothing, become cumbersome. In the advanced case we will use some community developed code that makes working with such spectral line image cubes a lot easier. Thing that come to mind are:\n",
    "* WCS (astronomical coordinate systems)\n",
    "* units (the flux unit in radio astronomy is Jy/beam)\n",
    "* arbitrary slices through the cube\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
